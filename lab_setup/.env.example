# Databricks External Location Setup Configuration
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# IMPORTANT: Never commit .env to git - it contains sensitive information

# =============================================================================
# AWS Configuration
# =============================================================================

# AWS CLI profile name (configured via: aws configure --profile <name>)
AWS_CLI_PROFILE="oneblink"

# AWS Region where resources will be created
AWS_REGION="us-east-1"

# AWS Account ID (leave empty to auto-detect from AWS CLI credentials)
AWS_ACCOUNT_ID=""

# =============================================================================
# S3 Bucket Configuration
# =============================================================================

# Name of the S3 bucket to use for external location (must already exist)
S3_BUCKET_NAME="your-bucket-name"

# Optional: Subfolder path within the bucket (e.g., "data/external")
# Leave empty to use the bucket root
S3_PREFIX=""

# =============================================================================
# IAM Role Configuration
# =============================================================================

# Name for the IAM role that will be created
IAM_ROLE_NAME="databricks-unity-catalog-access"

# =============================================================================
# Databricks Configuration
# =============================================================================

# Databricks CLI profile name (configured via: databricks configure --profile <name>)
DATABRICKS_PROFILE=""

# Name for the storage credential in Databricks Unity Catalog
STORAGE_CREDENTIAL_NAME="my-s3-storage-credential"

# Name for the external location in Databricks Unity Catalog
EXTERNAL_LOCATION_NAME="my-s3-external-location"

# =============================================================================
# Serverless Mode (used by auto_scripts/databricks-setup)
# =============================================================================

# Set to "true" to skip cluster creation and use SQL Warehouse instead
# This is faster and more cost-effective for lakehouse table creation
USE_SERVERLESS="false"

# SQL Warehouse name (used when USE_SERVERLESS=true)
# The "Starter Warehouse" is available in most workspaces
WAREHOUSE_NAME="Starter Warehouse"

# Timeout for SQL statement execution in seconds
WAREHOUSE_TIMEOUT="600"

# =============================================================================
# Cluster Configuration (used by setup_databricks.sh and auto_scripts when USE_SERVERLESS=false)
# =============================================================================

# Cloud provider: "aws" or "azure"
CLOUD_PROVIDER="aws"

# AWS instance profile ARN for cluster nodes (registered in Databricks workspace)
# The setup script will auto-register this if not already registered
INSTANCE_PROFILE_ARN=""

# Node type for the cluster (auto-selected based on CLOUD_PROVIDER if empty)
# AWS default: m5.xlarge (16 GB, 4 cores)
# Azure default: Standard_D4ds_v5 (16 GB, 4 cores)
NODE_TYPE=""

# Databricks runtime version
SPARK_VERSION="17.3.x-cpu-ml-scala2.13"

# Runtime engine: "STANDARD" or "PHOTON"
# Photon disabled by default - only benefits large workloads (>100GB) and charges 2x DBUs
RUNTIME_ENGINE="STANDARD"

# Cluster auto-termination in minutes
AUTOTERMINATION_MINUTES="30"

# =============================================================================
# Optional Settings
# =============================================================================

# KMS Key ARN for server-side encryption (leave empty if not using KMS)
KMS_KEY_ARN=""

# Enable file events for change data capture (true/false)
ENABLE_FILE_EVENTS="true"

# Read-only access (true/false)
# Set to true if you only need read access to the S3 bucket
READ_ONLY="false"

# =============================================================================
# Unity Catalog Master Role (DO NOT CHANGE unless using GovCloud)
# =============================================================================
# Commercial AWS (default):
UNITY_CATALOG_MASTER_ROLE="arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL"

# GovCloud (uncomment if using GovCloud):
# UNITY_CATALOG_MASTER_ROLE="arn:aws-us-gov:iam::044793339203:role/unity-catalog-prod-UCMasterRole-1QRFA8SGY15OJ"

# GovCloud DoD (uncomment if using GovCloud DoD):
# UNITY_CATALOG_MASTER_ROLE="arn:aws-us-gov:iam::170661010020:role/unity-catalog-prod-UCMasterRole-1DI6DL6ZP26AS"
